{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHMOW17BWdMK"
      },
      "source": [
        "!pip install imageio git+https://github.com/tensorflow/docs XlsxWriter tensorflow_addons mplfinance yfinance &> /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odhau-aCfhSe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ea5c7f2-5a75-40f7-c0d9-48d215b466e6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8w3_pczWcGq"
      },
      "source": [
        "# from google.colab import auth\n",
        "# auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pV_o0HjDWema"
      },
      "source": [
        "# !mkdir historical\n",
        "# !gsutil -m cp -n gs://ganstick_project/historical/*.png historical/ &> /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wo3boB08IdO"
      },
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "import yfinance as yf\n",
        "import mplfinance as mpf\n",
        "\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "matplotlib.use('Agg') # remain in headless environment\n",
        "\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "\n",
        "import keras\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Layer\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import GRU\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Input\n",
        "from keras.layers import LayerNormalization\n",
        "from keras.layers import MultiHeadAttention\n",
        "from keras.layers import Conv1D\n",
        "from keras.layers import Softmax\n",
        "from keras.layers import Permute\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Lambda\n",
        "from keras.layers import Dot\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Concatenate\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import LayerNormalization\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "\n",
        "\n",
        "from keras.layers import *\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.set_printoptions(suppress=True)"
      ],
      "metadata": {
        "id": "igxPrWmA2wct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "candle_style = mpf.make_marketcolors(up='green', down='red', edge='inherit', wick='inherit')\n",
        "make_style = mpf.make_mpf_style(marketcolors=candle_style)\n",
        "\n",
        "# create candlestick images from selected dates of specific stocks\n",
        "fig = mpf.figure()\n",
        "def generate_candlestick_images(df):\n",
        "    fname = 'current_image.png'\n",
        "    mpf.plot(df, type='candle', figsize=(0.56, 0.56), savefig=fname, style=make_style, ylabel='', scale_padding=0.0, scale_width_adjustment=dict(candle=0.8), axisoff=True)\n",
        "    fig.clf()\n",
        "    return fname"
      ],
      "metadata": {
        "id": "PAkxg0heYqO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmHfeqaStRJ4"
      },
      "source": [
        "macro_data = []\n",
        "macro_csvs = list(glob.glob(\"drive/MyDrive/ganstick_project/lstm_data/*.csv\"))\n",
        "for f in macro_csvs:\n",
        "  name = f.split(\"/\")[-1].split(\".\")[0]\n",
        "  df = pd.read_csv(f)\n",
        "  df['date'] = pd.to_datetime(df['date'])\n",
        "  df.set_index('date', inplace=True)\n",
        "  df.sort_index(inplace=True)\n",
        "  # cutting off where our covid data ends/no longer compiled sources for it.\n",
        "  df = df.loc[df.index <= '2021-03-01']\n",
        "  if name == 'vix':\n",
        "    vix = df\n",
        "    vix['rolling_vix_mean'] = vix['open'].rolling(window=10).mean().fillna(0)\n",
        "    vix = vix[['rolling_vix_mean']]\n",
        "    # some duplicate in the download data\n",
        "    vix = vix[~vix.index.duplicated(keep='first')]\n",
        "    vix = vix.set_index(vix.index).resample('D').ffill()\n",
        "    macro_data.append(vix)\n",
        "  elif name == 'cpi':\n",
        "    cpi = df\n",
        "    cpi = cpi.set_index(cpi.index).resample('D').ffill()\n",
        "    macro_data.append(cpi)\n",
        "  elif name == 'civpart':\n",
        "    civpart = df\n",
        "    civpart = civpart.set_index(civpart.index).resample('D').ffill()\n",
        "    macro_data.append(civpart)\n",
        "  elif name == 'homepriceindex':\n",
        "    homepriceindex = df\n",
        "    homepriceindex = homepriceindex.set_index(homepriceindex.index).resample('D').ffill()\n",
        "    macro_data.append(homepriceindex)\n",
        "  elif name == 'covidrates':\n",
        "    covidrates = df\n",
        "    keep_cols = ['death', 'hospitalizedCurrently']\n",
        "    covidrates = covidrates[keep_cols]\n",
        "    covidrates.columns = ['covid_deaths', 'covid_current_hospitalized']\n",
        "    \n",
        "    # cutting off ends where covid data is either lacking or non-existent/inaccurate\n",
        "    covidrates = covidrates.loc[covidrates.index > '2020-03-17']\n",
        "    macro_data.append(covidrates)\n",
        "  elif name == 'fedfunds':\n",
        "    fedfunds = df\n",
        "    fedfunds = fedfunds.set_index(fedfunds.index).resample('D').ffill()\n",
        "    macro_data.append(fedfunds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# handling treasury yield spread separately (to not include in the overall list)\n",
        "for f in macro_csvs:\n",
        "  name = f.split(\"/\")[-1].split(\".\")[0]\n",
        "  df = pd.read_csv(f)\n",
        "  df['date'] = pd.to_datetime(df['date'])\n",
        "  df.set_index('date', inplace=True)\n",
        "  df.sort_index(inplace=True)\n",
        "  # cutting off where our covid data ends/no longer compiled sources for it.\n",
        "  df = df.loc[df.index <= '2021-03-01']\n",
        "  if name == '30yearrates':\n",
        "    thirty_year_rates = df\n",
        "    thirty_year_rates = thirty_year_rates.set_index(thirty_year_rates.index).resample('D').ffill()\n",
        "    thirty_year_rates.columns = ['thirty_year_rates']\n",
        "  elif name == '10yearrates':\n",
        "    ten_year_rates = df\n",
        "    ten_year_rates = ten_year_rates.set_index(ten_year_rates.index).resample('D').ffill()\n",
        "    ten_year_rates.columns = ['ten_year_rates']\n",
        "    \n",
        "# get 10-30 year spread\n",
        "first_common_date = \"1987-01-02\" # universal beginning date\n",
        "lastdate = \"2021-03-01\"\n",
        "yield_spread_df = pd.DataFrame()\n",
        "tenyear = ten_year_rates.loc[(ten_year_rates.index > first_common_date) & (ten_year_rates.index < lastdate)].copy()\n",
        "thirtyyear = thirty_year_rates.loc[(thirty_year_rates.index > first_common_date) & (thirty_year_rates.index < lastdate)].copy()\n",
        "yield_spread_df['spread'] = thirtyyear['thirty_year_rates'] - tenyear['ten_year_rates']"
      ],
      "metadata": {
        "id": "J9TM8WIZhuBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_historical(ticker):\n",
        "\tstock = yf.Ticker(ticker)\n",
        "\tstock_df = pd.DataFrame(stock.history(period='max', interval='1D'))\n",
        "\tstock_df.index = pd.to_datetime(stock_df.index).strftime('%Y-%m-%d')\n",
        "\n",
        "\t# only close to close\n",
        "\tstock_df.drop(columns=['Dividends', 'Stock Splits'], inplace=True)\n",
        " \n",
        "\tBEGINNING_DATE = \"1987-01-02\"\n",
        "\t# BEGINNING_DATE = \"2019-12-01\"\n",
        "\tFINAL_DATE = \"2021-03-01\"\n",
        "\n",
        "\t# the latest common date is 1987-01-02\n",
        "\tstock_df = stock_df.loc[(stock_df.index > BEGINNING_DATE) & (stock_df.index <= FINAL_DATE)]\n",
        "\tprint(stock_df.index[0], stock_df.index[-1])\n",
        "\tstock_df.index = pd.to_datetime(stock_df.index)\n",
        "\treturn stock_df\n",
        "\n",
        "# updated version (future five days)\n",
        "# create windowed version of dataset, set look_back to be the skip length (in days)\n",
        "def create_dataset(dataset, cnn, col_indexes, pred_col_index, look_back=1):\n",
        "\tx_data, y_data = [], []\n",
        "\tfor i in range(len(dataset)-look_back-5-1): # 5 day buffer for future days\n",
        "\t\ta = dataset[i:(i+look_back)][:, col_indexes] # chain indexing for non-contiguous columns\n",
        "\n",
        "\t\tohlc = pd.DataFrame(a[:, [0, 1, 2, 3]], columns=['Open', 'High', 'Low', 'Close'])\n",
        "\t\tohlc.index = pd.date_range(start=\"1990-01-01\", end=\"1990-01-05\") # dummy dates, only needed for mplfinance\n",
        "\n",
        "\t\tfname = generate_candlestick_images(ohlc)\n",
        "\t\timg = load_img(fname)\n",
        "\t\timg_array = img_to_array(img)[None, :, :]\n",
        "\n",
        "\t\tfeatures = cnn.predict(img_array)\n",
        "\t\tfeatures = features[0, 0, :] # (:, :, features)\n",
        "\t\t# duplication of features for each day\n",
        "\t\tfeatures = np.tile(features, (lookback, 1))\n",
        "\n",
        "\t\ta = np.concatenate((a, features), axis=1)\n",
        "\t\tb = dataset[(i+look_back):(i+look_back+5)][:, pred_col_index]\n",
        "  \n",
        "\t\tx_data.append(a)\n",
        "\t\ty_data.append(b)\n",
        "\treturn np.array(x_data), np.array(y_data).astype(int)\n",
        " \n",
        "def bollinger(close_price, sma, sigma, alpha):\n",
        "\t'''\n",
        "\tbollinger bands\n",
        "\t@param int alpha:         multiplicative factor for standard deviations\n",
        "\t@param float sigma:       standard deviation\n",
        "\t@param float sma: simple  moving average\n",
        "\t@param float close_price: current close price (k + 1) \n",
        "\t'''\n",
        "\tif close_price > (sma + alpha * sigma):\n",
        "\t\treturn 1\n",
        "\telif close_price < (sma - alpha * sigma):\n",
        "\t\treturn 1\n",
        "\treturn 0\n",
        "\n",
        "# set boolean for whether current price is greater than mean of previous (lookback) number of days\n",
        "def trend(df, lookback, alpha):\n",
        "\t'''\n",
        "\tbased on look-back period, set boolean for exceeding volatility \n",
        "\t'''\n",
        "\tnew_df = df.copy()\n",
        "\tnew_df['up_down'] = new_df['Close'].rolling(lookback + 1, min_periods=1).apply(lambda x: bollinger(x[-1], x[:-1].mean(), x[:-1].std(), alpha)).astype(int)\n",
        "\treturn new_df\n",
        "\n",
        "def build_cnn():\n",
        "    input_image = Input(shape=(56, 56, 3))\n",
        "\n",
        "    feature = Conv2D(64, (5, 5), strides=(3, 3), padding='valid')(input_image)\n",
        "    feature = Conv2D(128, (3, 3), strides=(3, 3), padding='valid')(feature)\n",
        "    feature = Conv2D(256, (3, 3), strides=(3, 3), padding='valid')(feature)\n",
        "    feature = MaxPooling2D((2, 2), strides=(2, 2))(feature)\n",
        "\n",
        "    model = Model(inputs=input_image, outputs=feature)\n",
        "    return model\n",
        "\n",
        "def build_df(df, lookback, alpha):\n",
        "    for macro_df in macro_data:\n",
        "        df = df.join(macro_df)\n",
        "    df = df.join(yield_spread_df)\n",
        "\n",
        "    # divide sequence into only 2: non-covid and covid sequences\n",
        "    covid_start = covidrates.index[0]\n",
        "\n",
        "    # split df on dates\n",
        "    precovid_df = df.loc[df.index < covid_start].copy()\n",
        "    precovid_df.drop(columns=['covid_deaths', 'covid_current_hospitalized'], inplace=True)\n",
        "\n",
        "    # # for covid only \n",
        "    # covid_df = df.loc[df.index >= covid_start].copy()\n",
        "\n",
        "    data = precovid_df.copy()\n",
        "\n",
        "    # add our boolean up or down by lookback days\n",
        "    data = trend(data, lookback, alpha)\n",
        "    dataset = data.to_numpy()\n",
        "\n",
        "    split_ratio = 3.3\n",
        "    split_index = int(len(dataset)//split_ratio)\n",
        "\n",
        "    dataset_train = dataset[split_index:]\n",
        "    dataset_test = dataset[:split_index]\n",
        "\n",
        "    return dataset_train, dataset_test, data"
      ],
      "metadata": {
        "id": "XB1mchO1aYwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lookback = 5\n",
        "alpha = 1.5\n",
        "cnn = build_cnn()\n",
        "\n",
        "def build_array(ticker):\n",
        "    print(\"working on:\", ticker)\n",
        "    df = get_historical(ticker)\n",
        "    dataset_train, dataset_test, data = build_df(df, lookback, alpha)\n",
        "    \n",
        "    y_col_index = len(data.columns) - 1\n",
        "    x_col_indexes = [i for i in range(y_col_index)]\n",
        "\n",
        "    x_train, y_train = create_dataset(dataset_train, cnn, x_col_indexes, y_col_index, lookback)\n",
        "    x_test, y_test = create_dataset(dataset_test, cnn, x_col_indexes, y_col_index, lookback)\n",
        "\n",
        "    return x_train, y_train, x_test, y_test"
      ],
      "metadata": {
        "id": "4E1ZHmFFZovp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tech = ['MSFT', 'AAPL', 'INTC','XOM', 'WMT', 'BP']\n",
        "tech = ['MSFT', 'XOM', 'AAPL']\n",
        "techtest = ['MSFT']\n",
        "\n",
        "x_train, y_train, x_test, y_test = map(np.vstack, zip(*map(build_array, tech)))"
      ],
      "metadata": {
        "id": "dmNH6mOC1fDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape, y_train.shape, y_train[0])"
      ],
      "metadata": {
        "id": "Pvx5OkUyqkUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_std, x_test_std, = x_train.copy(), x_test.copy()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# scaler only accepts 2 dim, so we need to flatten the last dim(the dim with columns we want to standardize) before passing to scaler\n",
        "# then reshape it back to original\n",
        "x_train_std = scaler.fit_transform(x_train_std.reshape(-1, x_train_std.shape[-1])).reshape(x_train_std.shape)\n",
        "x_test_std = scaler.transform(x_test_std.reshape(-1, x_test_std.shape[-1])).reshape(x_test_std.shape)"
      ],
      "metadata": {
        "id": "FpJiHJjyFmiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCWihIyCtw8k"
      },
      "source": [
        "'''\n",
        "Walk-Foward Validation\n",
        "Input                   Predict\n",
        "[t-4 . . . t]          [t+1 . . . t+5]\n",
        "[t-9 . . . t]          [t+1 . . . t+5]\n",
        "[t-14 . . . t]         [t+1 . . . t+5]\n",
        "\n",
        "In our case, we have to set timesteps to multiples of 5 (5-day candlestick patterns)\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75K9UEwmBwCx"
      },
      "source": [
        "# shape=[samples, timesteps, features]\n",
        "samples = x_train_std.shape[0]\n",
        "timesteps = lookback\n",
        "features = x_train_std.shape[-1]\n",
        "\n",
        "x_train_std = np.reshape(x_train_std, (samples, timesteps, features))\n",
        "\n",
        "samples = x_test_std.shape[0]\n",
        "timesteps = lookback\n",
        "features = x_test_std.shape[-1]\n",
        "\n",
        "x_test_std = np.reshape(x_test_std, (samples, timesteps, features))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ENNfQZ7-Zj-"
      },
      "source": [
        "x_train_std = x_train_std.astype('float32')\n",
        "x_test_std = x_test_std.astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train_std.shape, y_train.shape)"
      ],
      "metadata": {
        "id": "GZJWaXjuDK2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zNrsXmWXjnX"
      },
      "source": [
        "n_timesteps, n_features, n_outputs = x_train_std.shape[1], x_train_std.shape[2], y_train.shape[1]\n",
        "\n",
        "def lstm():\n",
        "    input_feature = Input(shape=(n_timesteps, n_features))\n",
        "\n",
        "    feature = Bidirectional(LSTM(40, activation='relu', input_shape=(n_timesteps, n_features), return_sequences=False))(input_feature)\n",
        "    feature = RepeatVector(n_outputs)(feature)\n",
        "\n",
        "    feature = Bidirectional(LSTM(n_outputs, return_sequences=True))(feature)\n",
        "    feature = TimeDistributed(Dense(1))(feature)\n",
        "    out = Activation('sigmoid')(feature)\n",
        "\n",
        "    model = keras.Model(inputs=input_feature, outputs=out)\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwaJenDpLg6E"
      },
      "source": [
        "from keras import metrics\n",
        "# MSE = keras.losses.MeanSquaredError()\n",
        "# MAE = keras.losses.MeanAbsoluteError()\n",
        "\n",
        "CC = keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "# sgd_opt = keras.optimizers.SGD(learning_rate=4e-4)\n",
        "\n",
        "# BCE = keras.losses.BinaryCrossentropy()\n",
        "# rmsprop_opt = keras.optimizers.RMSprop(learning_rate=1e-4)\n",
        "\n",
        "# learning rate reduce\n",
        "# rmsprop_opt = keras.optimizers.RMSprop(learning_rate=1e-6)\n",
        "adam_opt = keras.optimizers.Adam(learning_rate=1e-7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy7ErVd7C4Ha"
      },
      "source": [
        "model = lstm()\n",
        "# model = lstm_attention()\n",
        "# model = gru()\n",
        "model.compile(loss=CC, optimizer=adam_opt, metrics=[metrics.Accuracy()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "poG1ZpK4qAFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpsH-sAePZFq"
      },
      "source": [
        "history = model.fit(x_train_std, y_train, epochs=1000, batch_size=24, verbose=2, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# need this to show plot in colab\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "Xdla3U7gtxTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOeY434YPqzc"
      },
      "source": [
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(x_test_std, y_test, batch_size=24, verbose=2)\n",
        "print(\"Testing loss: \", results[0])\n",
        "print(\"Testing accuracy: \", results[1])"
      ],
      "metadata": {
        "id": "3m7x_5pjxlIU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}